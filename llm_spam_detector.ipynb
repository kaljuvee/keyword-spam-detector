{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62523cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LLM Spam Detector\n",
    "\n",
    "A simplified spam detector that uses only OpenAI's LLM to detect keyword spamming\n",
    "in e-commerce product descriptions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57806133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0497dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI\n",
    "import openai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2635e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b0585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdabbf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAISpamDetector:\n",
    "    \"\"\"Spam detector using OpenAI's LLM with few-shot learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\"):\n",
    "        self.model = model\n",
    "        self.system_prompt = \"\"\"\n",
    "        You are an expert at detecting keyword spam in e-commerce product descriptions.\n",
    "        \n",
    "        Keyword spamming is when sellers include unrelated, irrelevant keywords in their product descriptions\n",
    "        to improve their ranking in search results. This creates a poor experience for buyers who see irrelevant\n",
    "        products in their search results.\n",
    "        \n",
    "        Examples of keyword spam:\n",
    "        1. Adding a list of brand names that aren't related to the actual product\n",
    "        2. Adding hashtags with irrelevant terms\n",
    "        3. Including phrases like \"ignore:\" followed by keywords\n",
    "        4. Adding a long list of unrelated terms at the end of a description\n",
    "        \n",
    "        Your task is to analyze product descriptions and determine if they contain keyword spam.\n",
    "        Respond with a JSON object containing:\n",
    "        1. \"is_spam\": boolean indicating if the description contains keyword spam\n",
    "        2. \"spam_score\": a number between 0 and 1 indicating the confidence level\n",
    "        3. \"reasoning\": brief explanation of your decision\n",
    "        \"\"\"\n",
    "        \n",
    "        # Few-shot examples for better performance\n",
    "        self.few_shot_examples = [\n",
    "            {\n",
    "                \"description\": \"Super cute high waisted blue jeans. recommended for shorter / petite girls, I'm 5'1 and it fits me perfect. These are more of jeggings than jeans and have a polyester material inside. Only flaw: the zipper goes down by itself sometimes. Size 0 / waist 23\",\n",
    "                \"label\": \"Not Spam\"\n",
    "            },\n",
    "            {\n",
    "                \"description\": \"Jordan 5 P51 Camo Size 9 Good condition Soles have yellowing No box or lacelocks $160 Travis Supreme concepts Jordan 1 3 5 6 7 11 12 Nike ovo Kanye yeezy boost 350 shadow royal bred shattered cement top 3 black toe infrared raptor gamma space jam air max vapormax flyknit Travis Scott kaws off white atmos 95 97 98 silver gold bullet protro Kobe fieg kith Levi's undefeated Palace tinker stash SB dunk stone island Foamposite plus Acronym VF Wotherspoon 270 SW LeBron Kyrie Pippen\",\n",
    "                \"label\": \"Spam\"\n",
    "            },\n",
    "            {\n",
    "                \"description\": \"Denim jacket Never been worn Size XL\",\n",
    "                \"label\": \"Not Spam\"\n",
    "            },\n",
    "            {\n",
    "                \"description\": \"gorgeous y2k juicy couture velvet zip up. super pretty turquoise color. worn only a few times and in perfect condition. 100% authentic! size medium but would fit a small nicely tags #lingerie #lace #vintagedepop #sofuckingdepop #depopfamous lily rose depp devon lee carlson corset black tank cami 90s 2000s y2k tank top camisole black lace top mock lingerie black v neck lace lined top strappy top carrie bradshaw milkmaid cami crop top blouse v neck lace sex and the city cruel intentions clueless black swan victorias secret cami\",\n",
    "                \"label\": \"Spam\"\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def predict(self, texts: List[str], batch_size: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Predict if texts contain spam using OpenAI's LLM with few-shot learning.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings\n",
    "            batch_size: Number of examples to process in each API call\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries with prediction results\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Process in batches to avoid token limits\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            batch_results = self._predict_batch(batch_texts)\n",
    "            results.extend(batch_results)\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def _predict_batch(self, texts: List[str]) -> List[Dict]:\n",
    "        \"\"\"Process a batch of texts with the OpenAI API.\"\"\"\n",
    "        batch_results = []\n",
    "        \n",
    "        # Create few-shot examples content\n",
    "        few_shot_content = \"\\n\\n\".join([\n",
    "            f\"Description: {example['description']}\\nLabel: {example['label']}\"\n",
    "            for example in self.few_shot_examples\n",
    "        ])\n",
    "        \n",
    "        for text in texts:\n",
    "            try:\n",
    "                # Create prompt with few-shot examples\n",
    "                user_content = f\"\"\"\n",
    "                Here are some examples of spam and non-spam product descriptions:\n",
    "                \n",
    "                {few_shot_content}\n",
    "                \n",
    "                Now, analyze this product description for keyword spam:\n",
    "                \n",
    "                Description: {text}\n",
    "                \"\"\"\n",
    "                \n",
    "                response = openai.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": user_content}\n",
    "                    ],\n",
    "                    response_format={\"type\": \"json_object\"},\n",
    "                    temperature=0.1\n",
    "                )\n",
    "                \n",
    "                result = json.loads(response.choices[0].message.content)\n",
    "                batch_results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error with OpenAI API: {e}\")\n",
    "                # Fallback result\n",
    "                batch_results.append({\n",
    "                    \"is_spam\": False,\n",
    "                    \"spam_score\": 0.0,\n",
    "                    \"reasoning\": f\"Error processing with LLM: {str(e)}\"\n",
    "                })\n",
    "        \n",
    "        return batch_results\n",
    "    \n",
    "    def predict_binary(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Make binary predictions (spam/not spam).\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings\n",
    "            \n",
    "        Returns:\n",
    "            Array of binary predictions (0 for non-spam, 1 for spam)\n",
    "        \"\"\"\n",
    "        results = self.predict(texts)\n",
    "        return np.array([1 if result.get(\"is_spam\", False) else 0 for result in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ceac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"\n",
    "    Load data from CSV file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to CSV file\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (descriptions, labels)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df['description'].tolist(), df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55a50b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true: List[int], y_pred: List[int]) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate model performance.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    # Calculate accuracy\n",
    "    accuracy = sum(1 for true, pred in zip(y_true, y_pred) if true == pred) / len(y_true)\n",
    "    \n",
    "    # Calculate precision, recall, and F1 for spam class (1)\n",
    "    true_positives = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 1)\n",
    "    false_positives = sum(1 for true, pred in zip(y_true, y_pred) if true == 0 and pred == 1)\n",
    "    false_negatives = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 0)\n",
    "    \n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    true_negatives = sum(1 for true, pred in zip(y_true, y_pred) if true == 0 and pred == 0)\n",
    "    \n",
    "    print(\"\\nOpenAI LLM Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision (Spam): {precision:.4f}\")\n",
    "    print(f\"Recall (Spam): {recall:.4f}\")\n",
    "    print(f\"F1-Score (Spam): {f1_score:.4f}\")\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(\"                 Predicted\")\n",
    "    print(\"                 Not Spam    Spam\")\n",
    "    print(f\"Actual Not Spam    {true_negatives}         {false_positives}\")\n",
    "    print(f\"      Spam         {false_negatives}         {true_positives}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'confusion_matrix': [[true_negatives, false_positives], [false_negatives, true_positives]]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e2e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the LLM spam detection.\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    # Load a small subset of test data to save API costs\n",
    "    test_texts, test_labels = load_data('data/test_set.csv')\n",
    "    \n",
    "    # Use a smaller subset for testing\n",
    "    test_subset_size = min(20, len(test_texts))\n",
    "    test_subset_texts = test_texts[:test_subset_size]\n",
    "    test_subset_labels = test_labels[:test_subset_size]\n",
    "    \n",
    "    print(f\"Testing on {test_subset_size} examples from test data\")\n",
    "    \n",
    "    # OpenAI model\n",
    "    print(\"\\nEvaluating OpenAI model...\")\n",
    "    openai_detector = OpenAISpamDetector()\n",
    "    openai_predictions = openai_detector.predict_binary(test_subset_texts)\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(\"\\nDetailed Results:\")\n",
    "    print(f\"{'#':<3} {'Actual':<8} {'Predicted':<10} {'Description':<50}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for i, (text, true_label, pred_label) in enumerate(zip(test_subset_texts, test_subset_labels, openai_predictions)):\n",
    "        # Truncate text for display\n",
    "        short_text = text[:47] + \"...\" if len(text) > 50 else text\n",
    "        print(f\"{i+1:<3} {'Spam' if true_label == 1 else 'Not Spam':<8} {'Spam' if pred_label == 1 else 'Not Spam':<10} {short_text:<50}\")\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate_model(test_subset_labels, openai_predictions)\n",
    "    \n",
    "    print(\"\\nNote: OpenAI model was evaluated on a small subset of the test data to save API costs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81abdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
